---
title: "Human Activity Recognition: Machine Learning Prediction Assignment"
author: "Rohit Padebettu"
date: "11/11/2016"
output: html_document
---

## Synopsis

_In this project, we propose a machine learning based Stochastic Gradient Boosting classifier to classify the activities of 6 human subjects performing 5 different activities wearing accelerometers mounted on their waist, left thigh, right arm, and right ankle. The classifier uses a Gradient Boosted Algorithm of 950 decision trees with an interaction depth of 4 to get to an accuracy of 99.91%._

_In this experiment, six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)._ 

_More information about this can be found at [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har)._

## Data Source
The data used to build this machine learning classifier and the data for the prediction has been obtainined from the sources below:

* [Training Data Set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
* [Testing Data Set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The source for both these datasets is [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har)

## Loading the Data
We begin our analysis by first loading a set of R libraries which aid us in various steps of our analysis.

```{r LoadLibraries,echo= TRUE,warning=FALSE}
   suppressPackageStartupMessages(library(caret))      ## Workhorse for ML setup and algo selection
   suppressPackageStartupMessages(library(doParallel)) ## Library to allow parallel multicore processing
   suppressPackageStartupMessages(library(gbm))        ## Library for the ML algo used 
   suppressPackageStartupMessages(library(knitr))     ## Library for producing this document 
```

We then download the data from the links above into our working directory and load it into out workspace.
```{r LoadData, echo=TRUE}
   
    trainingurl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    #download.file(trainingurl,"pml-training.csv",method="curl")

    testingurl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    #download.file(testingurl,"pml-testing.csv",method="curl")

    training<-read.csv("pml-training.csv",stringsAsFactors = FALSE)
    testing<-read.csv("pml-testing.csv",stringsAsFactors = FALSE)

```

## Exploring and Processing the Data
Once the data is loaded into the system, we run some basic data exploration commands to get an idea about the size of the dataset and its structure.
```{r ExpData1, echo = TRUE}
    dim(training)
    dim(testing)
```
We see that the training set has `19622` observations and `160` attributes whereas the testing set has `20` observations and `160` attributes. The final attribute in the training data set is `classe` which gives the classification of the activity being performed by the subject identified by `user_name`. We will use this attribute to build and train our classifier.

The testing dataset doesn't have the `classe` attribute, but includes the `problem_id` attribute in its place. We will run our trained classifier on this dataset to predict the `classe` attribute for each of the 20 observations using the data from the other attributes for each `problem_id`

### Cleaning the data
We run the `str()` command on both these data sets (we don't print the results here to keep the document brief) and observe that a lot of the `160` attributes have missing `NA's` or `blanks`. We proceed to clean the training dataset to remove these attributes so as to commence our work on a relatively clean data set.

```{r TrainTransform1,echo=TRUE}
   # Converting the classe column to a factor in the training data set
   training$classe=as.factor(training$classe)
    
   # Removing cols with mostly NA's
   NACols<-apply(training,MARGIN = 2,FUN = function(X) sum(is.na(X)))
   training_sub<-training[,names(NACols[NACols<1000])]
   
   # Removing cols with mostly blank characters
   BlnkCols<-apply(training,MARGIN = 2,FUN = function(X) sum(X==""))
   BCols<-BlnkCols[!is.na(BlnkCols)]
   training_set<-training_sub[,names(BCols[BCols<1000])]
```    

Since we do not want our classifer to make its predictions based on the subject name or the time of the activity, we also remove the identifying attributes for subject and the time attributes. After this we end up with a set of about `54` attributes on which we can start building our model.

```{r TrainTransform2, echo = TRUE}

   # Removing raw timestamp and name columns
   remCols<-c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2",
              "cvtd_timestamp","new_window")
   training_set<-training_set[,!(names(training_set) %in% remCols)]
```

We further make the same transformations on our testing dataset so as to be able to make predictions using a consistent structure.

```{r TestTransform, echo=TRUE}   
   # Removing same cols from testing set
   testing_set<-subset(testing,select = c(names(training_set[,-c(54)]),"problem_id"))
```

## Building the Classifier

### Prepare training data
We prepare our `training_set` data for modelling by splitting it into two datasets:

- `p_training`
- `p_testing`

The `p_training` data set will be used to train and build our classifier, while the `p_testing` dataset will be used as on Out of Sample Test for the classifier **before** we test the classifier on the original `testing_set` which has no classification information.

```{r PrepTrainData, echo=TRUE}
   set.seed(1234)
   inTrain = createDataPartition(training_set$classe, p = 3/4)[[1]]
   p_training = training_set[ inTrain,]
   p_testing = training_set[-inTrain,]
```

### Setting Parallel Processing
Since we intend to run a large number of model iterations to be able to select the best performing model,it is quite possible for the training process to take hours to run if we ran it on a single core. We therefore make use of the `doParallel` package and parallel processing capabilities of `caret` package to run our simulations on multiple cores on our machine.

```{r ParallelProc, echo=TRUE}
  cl <- makeCluster(detectCores()-1)
  registerDoParallel(cl) 
```

### Gradient Boosting Model
We intend to use the Stochastic Gradient Boosting Model to build our classifier. *Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do. For more information please check out* [Wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting)

### Setting Tuning Parameters for GBM

* The `caret` package also allows us to `preprocess` the training data, which we use to `center` and `scale` the data in this case.
* The `caret` package allows us to set tuning parameters via `tuneGrid` to build multiple models with various iterations of the tuning parameters. 
* The `caret` package also allows us to control the training and perform `Repeated Cross Validations` via the `trainControl()` function. We perform 5-fold Cross Validation and repeat it 3 times each in this case.

For more information on tuning options for the `caret` package please visit [Max Kuhn Caret Package Tutorial](https://topepo.github.io/caret/model-training-and-tuning.html)

We prepare the setup for the model as shown in the code below. 

```{r ModelSetup, echo=TRUE}
  
  set.seed(1234)
  
  # Scaling and Centering the Data
  preprocess<-c("center", "scale")
  
  # Model Iterations
  myGbm <- expand.grid(n.trees = seq(250,1000,100),
                       interaction.depth = 2:4,
                       shrinkage = 0.05,
                       n.minobsinnode=20)
  
  # Cross Validation and Training Control
  trainctrlg <- trainControl(method="repeatedcv",
                             number=5,
                             repeats = 3)
  
```

### Training the Classifier
We train the `gbm` classifier using the `train()` function in `caret` package as below. We pass the Pre-Processing, Training Control and Tuning Grid options into the function itself. 

Given the number of iterations of the model we are requesting and the number of cross validations to perform, this takes about `25 mins` to train and select the best model on our system. The metric we use to select the best model is `Overall Accuracy`.

```{r TrainModel, echo=TRUE, eval=FALSE}
     system.time(
       model.gbm<-train(form = classe ~ .,
                        method = "gbm",
                        data = p_training,
                        metric="Accuracy",
                        preProcess = preprocess,
                        trControl = trainctrlg,
                        tuneGrid = myGbm
       )
   )
   stopCluster(cl)
   saveRDS(model.gbm,file="gbmmodel.rds")
```

### Classifier Statistics

Once the model is trained using the `p_training` dataset we can print the model to see the accuracy of the various iterations. The output also shows us that the `train` function has automatically selected the best model for us and what the characteristics of such model are.

**Note:** *To save time, we saved the model we generated using the above code to a file locally and on Github. We load it again here to perform the other downstream analysis*
```{r ModelPrint, echo=TRUE}
   model.gbm<-readRDS(file = "gbmmodel.rds")
   print(model.gbm)
```

We can also plot the model iterations and see how the accuracies of each model vary with the various tuning parameters.
```{r PlotModel, echo=TRUE, fig.height=7,fig.width=10 }
    plot(model.gbm)
```
We can see from the above data and plot that the selected model with 950 trees has an `Accuracy` of about **99.91%** with `Kappa` about **0.9989**

Finally we also plot the `Importance` of attributes in the model for the top 20 or so attributes
```{r PlotVarImp, echo=TRUE, fig.height=15,fig.width=10,warning=FALSE }
 dotPlot(varImp(model.gbm))
```

### Out of Sample Accuracy
To calculate the **Out Of Sample** accuracy of the trained model we predict the classifications using the `p_testing` set we created initially and generate the confusion matrix to compare against the actual classifications. **As can be seen, the accuracy of this classifier is very high on the testing set as well!**

```{r OOSTest, echo =TRUE}
   pred.gbm<-predict(model.gbm,p_testing)
   confusionMatrix(p_testing$classe,pred.gbm)
```

## Classifying the Testing set provided

Now that we have trained the classifier and also performed an out of sample test, we can now use it to perform the classification on the original unclassified testing set with `20 observations` that was provided. 

**Note:** *We has processed the original testing set to remove the columns with NA's, Blanks, Identifying information as well as time attributes. We called that the `testing_set`*

```{r FinalTest, echo = TRUE}
    pred.test.gbm<-predict(model.gbm,testing_set)
    summary(pred.test.gbm)
    
```

```{r TestTable, echo=TRUE}
    testing_pred<-cbind(testing_set,pred.test.gbm)
    kable(head(testing_pred[,c(54,55)],5))
```

## Conclusion
*We began by attempting to build and train a classifier to classify 5 different human activities of six subjects. We used the Stochastic Gradient Bossting model to build the classifier. We tested out a number of iterations of the parameters for the model and performed several cross validations to be able to select the best set of parameters and attributes. The resulting model performed had an accuracy of about 99.91% on the training set and had a perfect classification score on the out of sample testing set. The model was later applied to the unclassified testing set provided and scored a perfect 20/20 there as well!*